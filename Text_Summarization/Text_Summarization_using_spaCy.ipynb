{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOMhq3W4MMHv1I7mGOJstqd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/robinkm0610/NLP_Projects/blob/main/Text_Summarization/Text_Summarization_using_spaCy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Ib8x6r3Ar-il"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "from string import punctuation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get the text\n"
      ],
      "metadata": {
        "id": "WjOAAk5oszkd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "extra_words = list(STOP_WORDS) + list(punctuation) + ['\\n']"
      ],
      "metadata": {
        "id": "PmidEFqmsI8H"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(extra_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E3pZmN5JtOBy",
        "outputId": "050b0886-7fd8-466f-a166-f02412aad418"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['without', 'due', 'us', 'we', 'yourselves', 'eleven', 'mine', 'because', 'forty', 'all', 'much', 'that', 'also', 'anything', 'for', 'less', 'anyhow', 'afterwards', 'several', 'few', 'themselves', 'really', 'would', 'about', 'them', 'another', 'everything', 'do', 'doing', '‘re', 'meanwhile', 'below', 'serious', \"'ll\", 'seemed', 'yet', 'formerly', 'might', 'everywhere', 'regarding', 'indeed', 'neither', 'not', 'or', 'than', 'wherever', 'least', '‘m', 'becoming', 'none', 'both', 'across', 'amount', 'nor', 'never', 'fifteen', 'rather', 'throughout', 'what', 'could', 'therein', 'very', 'then', 'hereby', 'through', 'in', 'whenever', '‘ll', 'full', 'at', 'he', 'moreover', 'three', 'after', 'hereupon', 'further', 'yours', 'thereby', 'becomes', 'whither', 'keep', '‘s', 'ours', 'whereas', 'please', 'any', 'ever', 'i', 'with', 'can', 'although', 'from', \"'s\", 'yourself', 'nothing', 'front', 'five', 'whether', 'you', 'mostly', 'n‘t', 'its', 'by', 'here', 'alone', 'perhaps', 'already', 'whose', 'two', 'hence', 'still', 'while', 'well', 'those', 'n’t', 'put', 'next', 'an', 'see', 'always', 'own', 'most', 'seem', 'take', 'move', 'as', 'twenty', 'beside', 'whatever', 'sometimes', 'seems', 'have', 'before', 'quite', 'our', 'ca', 'say', 'such', 'and', 'beyond', 'up', 'became', \"'d\", 'there', 'back', 'me', 'otherwise', 'six', 'done', 'enough', 'former', 'against', 'does', 'be', 'nine', 'third', 'why', 'anywhere', 'amongst', 'down', 'thence', 'various', 'show', 'whereby', 'other', 'each', 'somehow', 'namely', 'noone', 'whereupon', 'him', 'nevertheless', 'latter', 'within', 'everyone', 'did', 'how', 'whoever', 'between', 'since', 'empty', 'someone', 'around', 'same', 'however', 'being', 'over', 'though', '’s', \"'m\", 're', \"n't\", 'elsewhere', 'sometime', 'along', 'out', 'off', 'besides', 'must', 'eight', 'give', 'anyone', 'even', 'side', 'herein', 'often', 'into', 'hers', \"'ve\", 'ten', 'towards', 'their', 'whole', 'upon', 'sixty', 'on', 'somewhere', 'only', 'nobody', 'should', 'among', 'once', 'some', 'this', 'go', 'will', 'of', 'per', 'made', 'used', 'these', 'had', 'your', 'am', 'name', 'behind', 'they', 'therefore', 'thereupon', '’m', 'are', 'she', 'to', 'now', 'except', 'is', 'one', 'ourselves', 'make', 'get', 'the', 'but', 'were', '’d', 'either', '’ve', '’re', 'first', 'whom', 'call', 'many', 'thereafter', '‘ve', 'beforehand', 'no', 'every', 'so', 'wherein', 'who', 'when', 'during', 'just', 'fifty', 'thus', 'hundred', 'himself', 'onto', 'nowhere', 'again', 'cannot', 'using', 'may', 'whence', '’ll', 'been', 'hereafter', 'was', 'latterly', 'until', 'itself', 'bottom', 'anyway', 'together', 'almost', 'become', 'her', 'if', 'where', 'myself', 'four', 'a', 'has', 'herself', 'above', 'his', 'via', 'others', 'it', 'too', 'my', 'more', 'which', \"'re\", 'unless', 'under', 'toward', 'thru', 'last', 'whereafter', 'seeming', '‘d', 'twelve', 'something', 'part', 'else', 'top', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~', '\\n']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load('en_core_web_sm')"
      ],
      "metadata": {
        "id": "Gr9aNhEztR6M"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc = doc=\"\"\"NLP is a way for computers to analyze, understand, and derive meaning from human language in a smart and useful way. By utilizing NLP, developers can organize and structure knowledge to perform tasks such as automatic summarization, translation, named entity recognition, relationship extraction, sentiment analysis, speech recognition, and topic segmentation.Apart from common word processor operations that treat text like a mere sequence of symbols, NLP considers the hierarchical structure of language: several words make a phrase, several phrases make a sentence and, ultimately, sentences convey ideas,” John Rehling, an NLP expert at Meltwater Group,said in How Natural Language Processing Helps Uncover Social Media Sentiment. “By analyzing language for its meaning, NLP systems have long filled useful roles, such as correcting grammar, converting speech to text and automatically translating between languages.NLP is used to analyze text, allowing machines to understand how human’s speak. This human-computer interaction enables real-world applications like automatic text summarization,sentiment analysis, topic extraction, named entity recognition, parts-of-speech tagging, relationship extraction,stemming, and more. NLP is commonly used for text mining, machine translation, and automated question answering.NLP is characterized as a hard problem in computer science. Human language is rarely precise, or plainly spoken.To understand human language is to understand not only the words, but the concepts and how they’re linked together to create meaning. Despite language being one of the easiest things for humans to learn, the ambiguity of language is what makes natural language processing a difficult problem for computers to master.\"\"\"\n"
      ],
      "metadata": {
        "id": "xFvJFKF0EV35"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docx = nlp(doc)"
      ],
      "metadata": {
        "id": "YNtCws4ZEcFB"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remove extra words and get word count"
      ],
      "metadata": {
        "id": "mDu4g3M5EgwX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_words = [word.text for word in docx]\n"
      ],
      "metadata": {
        "id": "WUpHI_qdEfKv"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Freq_words = {}\n",
        "for w in all_words:\n",
        "  w1 = w.lower()\n",
        "  if w1 not in extra_words:\n",
        "    if w1 in Freq_words.keys():\n",
        "      Freq_words[w1] +=1\n",
        "    else:\n",
        "      Freq_words[w1] = 1"
      ],
      "metadata": {
        "id": "oKZYfmzrFILe"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Freq_words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kG6xl0h7FwRm",
        "outputId": "ec348aa4-a81b-4725-9e49-18d455d4af38"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'nlp': 8,\n",
              " 'way': 2,\n",
              " 'computers': 2,\n",
              " 'analyze': 2,\n",
              " 'understand': 4,\n",
              " 'derive': 1,\n",
              " 'meaning': 3,\n",
              " 'human': 5,\n",
              " 'language': 9,\n",
              " 'smart': 1,\n",
              " 'useful': 2,\n",
              " 'utilizing': 1,\n",
              " 'developers': 1,\n",
              " 'organize': 1,\n",
              " 'structure': 2,\n",
              " 'knowledge': 1,\n",
              " 'perform': 1,\n",
              " 'tasks': 1,\n",
              " 'automatic': 2,\n",
              " 'summarization': 2,\n",
              " 'translation': 2,\n",
              " 'named': 2,\n",
              " 'entity': 2,\n",
              " 'recognition': 3,\n",
              " 'relationship': 2,\n",
              " 'extraction': 3,\n",
              " 'sentiment': 3,\n",
              " 'analysis': 2,\n",
              " 'speech': 3,\n",
              " 'topic': 2,\n",
              " 'segmentation': 1,\n",
              " 'apart': 1,\n",
              " 'common': 1,\n",
              " 'word': 1,\n",
              " 'processor': 1,\n",
              " 'operations': 1,\n",
              " 'treat': 1,\n",
              " 'text': 5,\n",
              " 'like': 2,\n",
              " 'mere': 1,\n",
              " 'sequence': 1,\n",
              " 'symbols': 1,\n",
              " 'considers': 1,\n",
              " 'hierarchical': 1,\n",
              " 'words': 2,\n",
              " 'phrase': 1,\n",
              " 'phrases': 1,\n",
              " 'sentence': 1,\n",
              " 'ultimately': 1,\n",
              " 'sentences': 1,\n",
              " 'convey': 1,\n",
              " 'ideas': 1,\n",
              " '”': 1,\n",
              " 'john': 1,\n",
              " 'rehling': 1,\n",
              " 'expert': 1,\n",
              " 'meltwater': 1,\n",
              " 'group': 1,\n",
              " 'said': 1,\n",
              " 'natural': 2,\n",
              " 'processing': 2,\n",
              " 'helps': 1,\n",
              " 'uncover': 1,\n",
              " 'social': 1,\n",
              " 'media': 1,\n",
              " '“': 1,\n",
              " 'analyzing': 1,\n",
              " 'systems': 1,\n",
              " 'long': 1,\n",
              " 'filled': 1,\n",
              " 'roles': 1,\n",
              " 'correcting': 1,\n",
              " 'grammar': 1,\n",
              " 'converting': 1,\n",
              " 'automatically': 1,\n",
              " 'translating': 1,\n",
              " 'languages': 1,\n",
              " 'allowing': 1,\n",
              " 'machines': 1,\n",
              " 'speak': 1,\n",
              " 'computer': 2,\n",
              " 'interaction': 1,\n",
              " 'enables': 1,\n",
              " 'real': 1,\n",
              " 'world': 1,\n",
              " 'applications': 1,\n",
              " 'parts': 1,\n",
              " 'tagging': 1,\n",
              " 'stemming': 1,\n",
              " 'commonly': 1,\n",
              " 'mining': 1,\n",
              " 'machine': 1,\n",
              " 'automated': 1,\n",
              " 'question': 1,\n",
              " 'answering': 1,\n",
              " 'characterized': 1,\n",
              " 'hard': 1,\n",
              " 'problem': 2,\n",
              " 'science': 1,\n",
              " 'rarely': 1,\n",
              " 'precise': 1,\n",
              " 'plainly': 1,\n",
              " 'spoken': 1,\n",
              " 'concepts': 1,\n",
              " 'linked': 1,\n",
              " 'create': 1,\n",
              " 'despite': 1,\n",
              " 'easiest': 1,\n",
              " 'things': 1,\n",
              " 'humans': 1,\n",
              " 'learn': 1,\n",
              " 'ambiguity': 1,\n",
              " 'makes': 1,\n",
              " 'difficult': 1,\n",
              " 'master': 1}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get Title\n"
      ],
      "metadata": {
        "id": "FYVttFJOF2VS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "val = sorted(Freq_words.values())\n",
        "val"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D_B5ux7PFycI",
        "outputId": "b21a6a53-7fb0-465a-fba0-81f959e39ada"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 2,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 4,\n",
              " 5,\n",
              " 5,\n",
              " 8,\n",
              " 9]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_freq= val[-3:]\n",
        "print(\"Topic of document give :-\")\n",
        "for word, freq in Freq_words.items():\n",
        "  if freq in max_freq:\n",
        "    print(word, end = \" \")\n",
        "  else:\n",
        "    continue"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aZKj8i05GSH6",
        "outputId": "1a87ee7b-4380-447d-ff8b-83b89cbad44a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic of document give :-\n",
            "nlp human language text "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TF_IDF"
      ],
      "metadata": {
        "id": "Z2W_k1TqGxMs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for word in Freq_words.keys():\n",
        "  Freq_words[word] = (Freq_words[word]/max_freq[-1])"
      ],
      "metadata": {
        "id": "wrFEsAHmGmZ6"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Freq_words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jYd2WhMLHP2U",
        "outputId": "4a67d5eb-26f1-4b1a-e232-242f19d44a66"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'nlp': 0.8888888888888888,\n",
              " 'way': 0.2222222222222222,\n",
              " 'computers': 0.2222222222222222,\n",
              " 'analyze': 0.2222222222222222,\n",
              " 'understand': 0.4444444444444444,\n",
              " 'derive': 0.1111111111111111,\n",
              " 'meaning': 0.3333333333333333,\n",
              " 'human': 0.5555555555555556,\n",
              " 'language': 1.0,\n",
              " 'smart': 0.1111111111111111,\n",
              " 'useful': 0.2222222222222222,\n",
              " 'utilizing': 0.1111111111111111,\n",
              " 'developers': 0.1111111111111111,\n",
              " 'organize': 0.1111111111111111,\n",
              " 'structure': 0.2222222222222222,\n",
              " 'knowledge': 0.1111111111111111,\n",
              " 'perform': 0.1111111111111111,\n",
              " 'tasks': 0.1111111111111111,\n",
              " 'automatic': 0.2222222222222222,\n",
              " 'summarization': 0.2222222222222222,\n",
              " 'translation': 0.2222222222222222,\n",
              " 'named': 0.2222222222222222,\n",
              " 'entity': 0.2222222222222222,\n",
              " 'recognition': 0.3333333333333333,\n",
              " 'relationship': 0.2222222222222222,\n",
              " 'extraction': 0.3333333333333333,\n",
              " 'sentiment': 0.3333333333333333,\n",
              " 'analysis': 0.2222222222222222,\n",
              " 'speech': 0.3333333333333333,\n",
              " 'topic': 0.2222222222222222,\n",
              " 'segmentation': 0.1111111111111111,\n",
              " 'apart': 0.1111111111111111,\n",
              " 'common': 0.1111111111111111,\n",
              " 'word': 0.1111111111111111,\n",
              " 'processor': 0.1111111111111111,\n",
              " 'operations': 0.1111111111111111,\n",
              " 'treat': 0.1111111111111111,\n",
              " 'text': 0.5555555555555556,\n",
              " 'like': 0.2222222222222222,\n",
              " 'mere': 0.1111111111111111,\n",
              " 'sequence': 0.1111111111111111,\n",
              " 'symbols': 0.1111111111111111,\n",
              " 'considers': 0.1111111111111111,\n",
              " 'hierarchical': 0.1111111111111111,\n",
              " 'words': 0.2222222222222222,\n",
              " 'phrase': 0.1111111111111111,\n",
              " 'phrases': 0.1111111111111111,\n",
              " 'sentence': 0.1111111111111111,\n",
              " 'ultimately': 0.1111111111111111,\n",
              " 'sentences': 0.1111111111111111,\n",
              " 'convey': 0.1111111111111111,\n",
              " 'ideas': 0.1111111111111111,\n",
              " '”': 0.1111111111111111,\n",
              " 'john': 0.1111111111111111,\n",
              " 'rehling': 0.1111111111111111,\n",
              " 'expert': 0.1111111111111111,\n",
              " 'meltwater': 0.1111111111111111,\n",
              " 'group': 0.1111111111111111,\n",
              " 'said': 0.1111111111111111,\n",
              " 'natural': 0.2222222222222222,\n",
              " 'processing': 0.2222222222222222,\n",
              " 'helps': 0.1111111111111111,\n",
              " 'uncover': 0.1111111111111111,\n",
              " 'social': 0.1111111111111111,\n",
              " 'media': 0.1111111111111111,\n",
              " '“': 0.1111111111111111,\n",
              " 'analyzing': 0.1111111111111111,\n",
              " 'systems': 0.1111111111111111,\n",
              " 'long': 0.1111111111111111,\n",
              " 'filled': 0.1111111111111111,\n",
              " 'roles': 0.1111111111111111,\n",
              " 'correcting': 0.1111111111111111,\n",
              " 'grammar': 0.1111111111111111,\n",
              " 'converting': 0.1111111111111111,\n",
              " 'automatically': 0.1111111111111111,\n",
              " 'translating': 0.1111111111111111,\n",
              " 'languages': 0.1111111111111111,\n",
              " 'allowing': 0.1111111111111111,\n",
              " 'machines': 0.1111111111111111,\n",
              " 'speak': 0.1111111111111111,\n",
              " 'computer': 0.2222222222222222,\n",
              " 'interaction': 0.1111111111111111,\n",
              " 'enables': 0.1111111111111111,\n",
              " 'real': 0.1111111111111111,\n",
              " 'world': 0.1111111111111111,\n",
              " 'applications': 0.1111111111111111,\n",
              " 'parts': 0.1111111111111111,\n",
              " 'tagging': 0.1111111111111111,\n",
              " 'stemming': 0.1111111111111111,\n",
              " 'commonly': 0.1111111111111111,\n",
              " 'mining': 0.1111111111111111,\n",
              " 'machine': 0.1111111111111111,\n",
              " 'automated': 0.1111111111111111,\n",
              " 'question': 0.1111111111111111,\n",
              " 'answering': 0.1111111111111111,\n",
              " 'characterized': 0.1111111111111111,\n",
              " 'hard': 0.1111111111111111,\n",
              " 'problem': 0.2222222222222222,\n",
              " 'science': 0.1111111111111111,\n",
              " 'rarely': 0.1111111111111111,\n",
              " 'precise': 0.1111111111111111,\n",
              " 'plainly': 0.1111111111111111,\n",
              " 'spoken': 0.1111111111111111,\n",
              " 'concepts': 0.1111111111111111,\n",
              " 'linked': 0.1111111111111111,\n",
              " 'create': 0.1111111111111111,\n",
              " 'despite': 0.1111111111111111,\n",
              " 'easiest': 0.1111111111111111,\n",
              " 'things': 0.1111111111111111,\n",
              " 'humans': 0.1111111111111111,\n",
              " 'learn': 0.1111111111111111,\n",
              " 'ambiguity': 0.1111111111111111,\n",
              " 'makes': 0.1111111111111111,\n",
              " 'difficult': 0.1111111111111111,\n",
              " 'master': 0.1111111111111111}"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sentence Strength"
      ],
      "metadata": {
        "id": "2s84Xy_4HgvD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sent_strength = {}\n",
        "for sent in docx.sents:\n",
        "  for word in sent:\n",
        "    if word.text in Freq_words.keys():\n",
        "      if sent in sent_strength.keys():\n",
        "        #print(word.text.lower() ,\" \" ,Freq_words[word.text.lower()])\n",
        "        sent_strength[sent] += Freq_words[word.text.lower()]\n",
        "      else:\n",
        "        sent_strength[sent]= Freq_words[word.text.lower()]\n",
        "    else:\n",
        "      continue"
      ],
      "metadata": {
        "id": "0X0AvssFHS4U"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent_strength"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uz9ztEBCIEdT",
        "outputId": "71291307-41d7-44de-a97b-ceb9ad2ca4f2"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{NLP is a way for computers to analyze, understand, and derive meaning from human language in a smart and useful way.: 3.666666666666667,\n",
              " By utilizing NLP, developers can organize and structure knowledge to perform tasks such as automatic summarization, translation, named entity recognition, relationship extraction, sentiment analysis, speech recognition, and topic segmentation.: 4.4444444444444455,\n",
              " Apart from common word processor operations that treat text like a mere sequence of symbols, NLP considers the hierarchical structure of language: several words make a phrase, several phrases make a sentence and, ultimately, sentences convey ideas,” John Rehling, an NLP expert at Meltwater Group,said in How Natural Language Processing Helps Uncover Social Media Sentiment.: 4.444444444444444,\n",
              " “By analyzing language for its meaning, NLP systems have long filled useful roles, such as correcting grammar, converting speech to text and automatically translating between languages.: 3.777777777777778,\n",
              " NLP is used to analyze text, allowing machines to understand how human’s speak.: 2.111111111111111,\n",
              " This human-computer interaction enables real-world applications like automatic text summarization,sentiment analysis, topic extraction, named entity recognition, parts-of-speech tagging, relationship extraction,stemming, and more.: 5.666666666666666,\n",
              " NLP is commonly used for text mining, machine translation, and automated question answering.: 1.4444444444444446,\n",
              " NLP is characterized as a hard problem in computer science.: 0.7777777777777777,\n",
              " Human language is rarely precise, or plainly spoken.: 1.4444444444444446,\n",
              " To understand human language is to understand not only the words, but the concepts and how they’re linked together to create meaning.: 3.333333333333334,\n",
              " Despite language being one of the easiest things for humans to learn, the ambiguity of language is what makes natural language processing a difficult problem for computers to master.: 4.777777777777778}"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "top_sentences = (sorted(sent_strength.values())[::-1])"
      ],
      "metadata": {
        "id": "yQZu5SeEIHFh"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_sentences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RIQ7tmRPJpSD",
        "outputId": "ec984d89-1a0c-4ec1-826e-3406ba1fad56"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[5.666666666666666,\n",
              " 4.777777777777778,\n",
              " 4.4444444444444455,\n",
              " 4.444444444444444,\n",
              " 3.777777777777778,\n",
              " 3.666666666666667,\n",
              " 3.333333333333334,\n",
              " 2.111111111111111,\n",
              " 1.4444444444444446,\n",
              " 1.4444444444444446,\n",
              " 0.7777777777777777]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "top30percent_sentence = int(0.3*len(top_sentences))"
      ],
      "metadata": {
        "id": "ba9grfmPJrMj"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_sent = top_sentences[:top30percent_sentence]\n",
        "top_sent"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dp_Mks2MJ0Bx",
        "outputId": "0be1a65d-6038-4bca-b0cc-a172e56c1935"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[5.666666666666666, 4.777777777777778, 4.4444444444444455]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summary=[]\n",
        "for sent, strenght in sent_strength.items():\n",
        "  if strenght in top_sent:\n",
        "    summary.append(sent)\n",
        "  else:\n",
        "    continue"
      ],
      "metadata": {
        "id": "K2oAqVpZJ2Wo"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in summary:\n",
        "  print(i,end=\" \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D9p9u0ukKWkY",
        "outputId": "16462fae-6d92-414a-da8a-f1faf49ef4e6"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "By utilizing NLP, developers can organize and structure knowledge to perform tasks such as automatic summarization, translation, named entity recognition, relationship extraction, sentiment analysis, speech recognition, and topic segmentation. This human-computer interaction enables real-world applications like automatic text summarization,sentiment analysis, topic extraction, named entity recognition, parts-of-speech tagging, relationship extraction,stemming, and more. Despite language being one of the easiest things for humans to learn, the ambiguity of language is what makes natural language processing a difficult problem for computers to master. "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lk37psB2KZ2Z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}